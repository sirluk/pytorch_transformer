model_cfg_small:
  vocab_size: 32000
  context_length: 512
  model_dim: 512
  n_blocks: 8
  n_attn_heads: 8
  k_config: 'mha'
  v_config: 'mha'
  kv_groups: null
  ffn_hidden_dim: null
  dropout_prob: 0.0
  bias: False
  norm_eps: 1e-5
  tie_word_embeddings: False
  ffn_hidden_dim_multiple_of: 256
model_cfg:
  vocab_size: 32000
  context_length: 1024
  model_dim: 1024
  n_blocks: 16
  n_attn_heads: 16
  k_config: 'mha'
  v_config: 'mha'
  kv_groups: 4
  ffn_hidden_dim: null
  dropout_prob: 0.0
  bias: False
  norm_eps: 1e-5
  tie_word_embeddings: False
  ffn_hidden_dim_multiple_of: 256
model_cfg_llama7b:
  vocab_size: 32000
  context_length: 4096
  model_dim: 4096
  n_blocks: 32
  n_attn_heads: 32
  k_config: 'mha'
  v_config: 'mha'
  kv_groups: null
  ffn_hidden_dim: null # model_dim * 4
  dropout_prob: 0.0
  bias: False
  norm_eps: 1e-5
  tie_word_embeddings: False
  ffn_hidden_dim_multiple_of: 256
train_cfg:
  lr: 3e-4
  lr_min: 3e-5
  batch_size: 16
  adam_b1: 0.9
  adam_b2: 0.95
  adam_eps: 10e-6
  adam_weight_decay: 0.1
  gradient_accumulation_steps: 1
  max_grad: 1.0
  train_iters: 6e5
  warmup_iters: 2000
  eval_interval: 2000
  eval_iters: 200
  track_memory: True
  future_context_size: 16